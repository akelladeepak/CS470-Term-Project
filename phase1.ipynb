{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5a4fac",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bdcf92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "def read_arff_file(filepath, dimension_name):\n",
    "    \"\"\"\n",
    "    - Loads an ARFF into a DataFrame.\n",
    "    - Decodes any byte-strings to UTF-8 str.\n",
    "    - Attempts to cast numeric columns.\n",
    "    - Renames the ARFF 'class' or 'classAttribute' → 'class',\n",
    "      and prefixes all other attributes with '{dimension_name}_'.\n",
    "    \"\"\"\n",
    "    raw_data, _ = arff.loadarff(filepath)\n",
    "    df = pd.DataFrame(raw_data)\n",
    "\n",
    "    # 1) Decode bytes → str\n",
    "    for col in df.select_dtypes([object]):\n",
    "        df[col] = df[col].apply(\n",
    "            lambda x: x.decode('utf-8') if isinstance(x, bytes) else x\n",
    "        )\n",
    "\n",
    "    # 2) Numeric cast where possible\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "    # 3) Rename: unify any 'class' or 'classAttribute' to 'class';\n",
    "    #    prefix everything else with dimension_name_\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        low = col.lower()\n",
    "        if low == 'class' or low == 'classattribute':\n",
    "            new_cols.append('class')\n",
    "        else:\n",
    "            new_cols.append(f\"{dimension_name}_{col}\")\n",
    "    df.columns = new_cols\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938dafbc",
   "metadata": {},
   "source": [
    "# Phase 1: Combine NATOPS ARFF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12c62c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_and_sid(dfs):\n",
    "    \"\"\"\n",
    "    - Horizontally concat a list of DataFrames (one per dimension).\n",
    "    - Drop any duplicated columns.\n",
    "    - Insert a 1-based 'sid'.\n",
    "    \"\"\"\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat(dfs, axis=1)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    df.insert(0, 'sid', range(1, len(df) + 1))\n",
    "    return df\n",
    "\n",
    "def process_dataset_folder(folder_path, dataset_name):\n",
    "    \"\"\"\n",
    "    - Reads all .arff in folder_path.\n",
    "    - Splits them into train/test by filename.\n",
    "    - Builds a wide table per split, adds sid, then stacks train+test.\n",
    "    - Tags with a `dataset` column.\n",
    "    \"\"\"\n",
    "    arff_files = [\n",
    "        f for f in os.listdir(folder_path)\n",
    "        if f.lower().endswith('.arff')\n",
    "    ]\n",
    "    train_dfs, test_dfs = [], []\n",
    "\n",
    "    for fn in arff_files:\n",
    "        name_upper = fn.upper()\n",
    "        if 'TRAIN' in name_upper:\n",
    "            split = 'train'\n",
    "        elif 'TEST' in name_upper:\n",
    "            split = 'test'\n",
    "        else:\n",
    "            print(f\"⚠️  Skipping unrecognized file: {fn}\")\n",
    "            continue\n",
    "\n",
    "        base = os.path.splitext(fn)[0]\n",
    "        dimension = base.upper().replace('_TRAIN','').replace('_TEST','').lower()\n",
    "        df_dim = read_arff_file(os.path.join(folder_path, fn), dimension)\n",
    "        df_dim['split'] = split\n",
    "\n",
    "        if split == 'train':\n",
    "            train_dfs.append(df_dim)\n",
    "        else:\n",
    "            test_dfs.append(df_dim)\n",
    "\n",
    "    df_train = concat_and_sid(train_dfs)\n",
    "    df_test  = concat_and_sid(test_dfs)\n",
    "    df_all   = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "    # Quick sanity check\n",
    "    feature_cols = [\n",
    "        c for c in df_all.columns\n",
    "        if c not in ('sid','split','class')\n",
    "    ]\n",
    "    if not feature_cols:\n",
    "        raise RuntimeError(\"No feature columns found in wide DataFrame. \"\n",
    "                           \"Check that your .arff files were read correctly.\")\n",
    "    df_all['dataset'] = dataset_name\n",
    "    return df_all\n",
    "\n",
    "def melt_to_time_steps(df):\n",
    "    \"\"\"\n",
    "    Turn the wide table (one row per sid) into long form:\n",
    "    - one row per (sid, time_step)\n",
    "    - columns: sid, split, class, time_step, plus one column per dimension\n",
    "    \"\"\"\n",
    "    id_vars = [c for c in ('sid','split','class') if c in df.columns]\n",
    "\n",
    "    # all dimension prefixes (anything before the first '_', excluding id_vars+dataset)\n",
    "    dims = sorted({\n",
    "        col.split('_')[0]\n",
    "        for col in df.columns\n",
    "        if '_' in col and col.split('_')[0] not in id_vars + ['dataset']\n",
    "    })\n",
    "\n",
    "    if not dims:\n",
    "        raise RuntimeError(\"No dimensions detected—nothing to melt.\")\n",
    "\n",
    "    melted_dfs = []\n",
    "    for dim in dims:\n",
    "        prefix = f\"{dim}_\"\n",
    "        time_cols = [c for c in df.columns if c.startswith(prefix)]\n",
    "        if not time_cols:\n",
    "            continue\n",
    "\n",
    "        # stable sort by suffix, attempting numeric if possible\n",
    "        def sort_key(c):\n",
    "            suf = c[len(prefix):]\n",
    "            return int(suf) if suf.isdigit() else suf\n",
    "\n",
    "        time_cols = sorted(time_cols, key=sort_key)\n",
    "\n",
    "        m = df[id_vars + time_cols].melt(\n",
    "            id_vars=id_vars,\n",
    "            value_vars=time_cols,\n",
    "            var_name='time',\n",
    "            value_name=dim\n",
    "        )\n",
    "        m['time_step'] = m['time'].map({col: i\n",
    "                                         for i, col in enumerate(time_cols)})\n",
    "        m = m.drop(columns='time')\n",
    "        melted_dfs.append(m)\n",
    "\n",
    "    if not melted_dfs:\n",
    "        raise RuntimeError(\"After filtering, no dimension had time-step columns.\")\n",
    "\n",
    "    # merge them on sid/split/class/time_step\n",
    "    df_long = melted_dfs[0]\n",
    "    for df_next in melted_dfs[1:]:\n",
    "        df_long = pd.merge(\n",
    "            df_long, df_next,\n",
    "            on=['sid','split','class','time_step']\n",
    "        )\n",
    "    return df_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54f661",
   "metadata": {},
   "source": [
    "# Phase 2: Clustering & Atomic-Unit Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "25084f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(df, feature_cols, n_clusters, random_state=42):\n",
    "    \"\"\"\n",
    "    Runs KMeans on the numeric subset of feature_cols,\n",
    "    writes the cluster label into df['cluster'], and returns df.\n",
    "    \"\"\"\n",
    "    numeric = [c for c in feature_cols\n",
    "               if np.issubdtype(df[c].dtype, np.number)]\n",
    "    dropped = set(feature_cols) - set(numeric)\n",
    "    if dropped:\n",
    "        print(f\"Dropped non-numeric before clustering: {dropped}\")\n",
    "\n",
    "    X = df[numeric].to_numpy()\n",
    "    km = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "    df['cluster'] = km.fit_predict(X)\n",
    "    return df\n",
    "\n",
    "def compute_ratio_features(df, n_clusters):\n",
    "    \"\"\"\n",
    "    For each (split, sid, class) group, compute the normalized\n",
    "    counts of each cluster → cluster_i_ratio features.\n",
    "    \"\"\"\n",
    "    group_cols = [c for c in ('split','sid','class') if c in df.columns]\n",
    "    ratios = (\n",
    "        df\n",
    "        .groupby(group_cols)['cluster']\n",
    "        .value_counts(normalize=True)\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "    ratios.columns = [f\"cluster_{int(c)}_ratio\"\n",
    "                      for c in ratios.columns]\n",
    "    return ratios.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a84f03",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba793745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 (wide) sample:\n",
      "   sid  natopsdimension10_channel_9_0  natopsdimension10_channel_9_1  \\\n",
      "0    1                       0.599967                       0.597535   \n",
      "1    2                       0.622368                       0.622228   \n",
      "2    3                       0.588525                       0.588389   \n",
      "3    4                       0.576847                       0.576713   \n",
      "4    5                       0.717469                       0.722515   \n",
      "\n",
      "   natopsdimension10_channel_9_2  natopsdimension10_channel_9_3  \\\n",
      "0                       0.597007                       0.599099   \n",
      "1                       0.622004                       0.621909   \n",
      "2                       0.588164                       0.588034   \n",
      "3                       0.575015                       0.575267   \n",
      "4                       0.725107                       0.726653   \n",
      "\n",
      "   natopsdimension10_channel_9_4  natopsdimension10_channel_9_5  \\\n",
      "0                       0.606181                       0.620752   \n",
      "1                       0.621940                       0.622165   \n",
      "2                       0.587961                       0.587895   \n",
      "3                       0.572163                       0.555767   \n",
      "4                       0.709988                       0.712092   \n",
      "\n",
      "   natopsdimension10_channel_9_6  natopsdimension10_channel_9_7  \\\n",
      "0                       0.630438                       0.631059   \n",
      "1                       0.622169                       0.622187   \n",
      "2                       0.591413                       0.605781   \n",
      "3                       0.555370                       0.603450   \n",
      "4                       0.718585                       0.732054   \n",
      "\n",
      "   natopsdimension10_channel_9_8  ...  natopsdimension9_channel_8_43  \\\n",
      "0                       0.628942  ...                      -0.188206   \n",
      "1                       0.622277  ...                      -0.179747   \n",
      "2                       0.679733  ...                      -0.159422   \n",
      "3                       0.669051  ...                      -0.145222   \n",
      "4                       0.739200  ...                      -0.273186   \n",
      "\n",
      "   natopsdimension9_channel_8_44  natopsdimension9_channel_8_45  \\\n",
      "0                      -0.176391                      -0.151265   \n",
      "1                      -0.181748                      -0.173544   \n",
      "2                      -0.160593                      -0.161653   \n",
      "3                      -0.134774                      -0.155229   \n",
      "4                      -0.273613                      -0.273925   \n",
      "\n",
      "   natopsdimension9_channel_8_46  natopsdimension9_channel_8_47  \\\n",
      "0                      -0.149752                      -0.163843   \n",
      "1                      -0.167847                      -0.159090   \n",
      "2                      -0.163661                      -0.163518   \n",
      "3                      -0.148243                      -0.148955   \n",
      "4                      -0.271116                      -0.272523   \n",
      "\n",
      "   natopsdimension9_channel_8_48  natopsdimension9_channel_8_49  \\\n",
      "0                      -0.167192                      -0.149480   \n",
      "1                      -0.156384                      -0.154926   \n",
      "2                      -0.163756                      -0.164298   \n",
      "3                      -0.150318                      -0.147396   \n",
      "4                      -0.271146                      -0.270345   \n",
      "\n",
      "   natopsdimension9_channel_8_50  \\\n",
      "0                      -0.152398   \n",
      "1                      -0.153674   \n",
      "2                      -0.165037   \n",
      "3                      -0.150914   \n",
      "4                      -0.273293   \n",
      "\n",
      "                                natops_relationalAtt  dataset  \n",
      "0  [[-0.372758, -0.367844, -0.378445, -0.386751, ...   NATOPS  \n",
      "1  [[-0.54737, -0.546334, -0.549748, -0.546891, -...   NATOPS  \n",
      "2  [[-0.587062, -0.587322, -0.586417, -0.584654, ...   NATOPS  \n",
      "3  [[-0.514671, -0.51864, -0.521285, -0.522843, -...   NATOPS  \n",
      "4  [[-0.718601, -0.721093, -0.717955, -0.722386, ...   NATOPS  \n",
      "\n",
      "[5 rows x 1229 columns] \n",
      "\n",
      "Phase 1 (long – corrected) sample:\n",
      "   isTest      fea1      fea2      fea3      fea4      fea5      fea6  \\\n",
      "0       0 -0.372758 -1.821679 -0.846321  0.465208 -2.015072 -0.839242   \n",
      "1       0 -0.547370 -1.600105 -0.809446  0.556062 -1.669622 -0.748726   \n",
      "2       0 -0.587062 -1.755034 -0.648786  0.542660 -1.759520 -0.573142   \n",
      "3       0 -0.514671 -1.893971 -0.748957  0.571942 -1.988466 -0.745504   \n",
      "4       0 -0.718601 -2.153186 -0.859093  0.808480 -2.175653 -0.763930   \n",
      "\n",
      "       fea7      fea8      fea9  ...     fea17     fea18     fea19     fea20  \\\n",
      "0 -0.564097 -0.796225 -0.149604  ... -1.534954 -0.673190 -0.536343 -1.626957   \n",
      "1 -0.668990 -0.673415 -0.162021  ... -1.271334 -0.495517 -0.557755 -1.416602   \n",
      "2 -0.683921 -0.750000 -0.146066  ... -1.406456 -0.372397 -0.729303 -1.516087   \n",
      "3 -0.563803 -0.729743 -0.132236  ... -1.504254 -0.595728 -0.392323 -1.661124   \n",
      "4 -0.689536 -0.803106 -0.269982  ... -1.801784 -0.574368 -0.531636 -1.875322   \n",
      "\n",
      "      fea21     fea22     fea23     fea24  sid  class  \n",
      "0 -0.594337  0.619205 -1.771773 -0.810086    1    4.0  \n",
      "1 -0.849636  0.618919 -1.497652 -0.754927    2    3.0  \n",
      "2 -0.464015  0.417640 -1.549212 -0.564249    3    3.0  \n",
      "3 -0.780828  0.439658 -1.701396 -0.809376    4    4.0  \n",
      "4 -0.876736  0.634141 -1.956089 -0.797694    5    3.0  \n",
      "\n",
      "[5 rows x 27 columns] \n",
      "\n",
      "Phase 2 sample:\n",
      "   sid  class  cluster_0_ratio  cluster_1_ratio  cluster_2_ratio  \\\n",
      "0    1    4.0              0.5              0.5              0.0   \n",
      "1    2    3.0              0.0              0.0              0.0   \n",
      "2    2    5.0              0.0              1.0              0.0   \n",
      "3    3    3.0              0.0              0.0              0.0   \n",
      "4    3    6.0              0.0              0.0              0.0   \n",
      "\n",
      "   cluster_3_ratio  cluster_4_ratio  cluster_5_ratio  cluster_6_ratio  \\\n",
      "0              0.0              0.0              0.0              0.0   \n",
      "1              0.0              1.0              0.0              0.0   \n",
      "2              0.0              0.0              0.0              0.0   \n",
      "3              0.0              1.0              0.0              0.0   \n",
      "4              0.0              0.0              1.0              0.0   \n",
      "\n",
      "   cluster_7_ratio  cluster_8_ratio  cluster_9_ratio  \n",
      "0              0.0              0.0              0.0  \n",
      "1              0.0              0.0              0.0  \n",
      "2              0.0              0.0              0.0  \n",
      "3              0.0              0.0              0.0  \n",
      "4              0.0              0.0              0.0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "     # Phase 1: ingest wide\n",
    "    base_folder = os.path.join('Phase1_Data','NATOPS')\n",
    "    if not os.path.isdir(base_folder):\n",
    "        raise FileNotFoundError(f\"NATOPS folder not found at {base_folder}\")\n",
    "\n",
    "    df_wide = process_dataset_folder(base_folder, 'NATOPS')\n",
    "    print(\"Phase 1 (wide) sample:\")\n",
    "    print(df_wide.head(), \"\\n\")\n",
    "    df_wide.to_csv('Phase1_NATOPS_Combined_wide.csv', index=False)\n",
    "\n",
    "    # Phase 1: reshape to long\n",
    "    df_long = melt_to_time_steps(df_wide)\n",
    "\n",
    "    # --- Begin header-renaming block ---\n",
    "    # 1) Create isTest (1 if test, else 0)\n",
    "    df_long['isTest'] = (df_long['split'] == 'test').astype(int)\n",
    "\n",
    "    # 2) Drop split, time_step, and the extra 'natops' column\n",
    "    df_long = df_long.drop(columns=['split','time_step','dataset'], errors='ignore')\n",
    "\n",
    "    # 3) Rename natopsdimension1…natopsdimension24 → fea1…fea24\n",
    "    nat_cols = sorted(\n",
    "        [c for c in df_long.columns if c.startswith('natopsdimension')],\n",
    "        key=lambda x: int(x.replace('natopsdimension',''))\n",
    "    )\n",
    "    rename_dict = {col: f\"fea{i+1}\" for i, col in enumerate(nat_cols)}\n",
    "    df_long = df_long.rename(columns=rename_dict)\n",
    "\n",
    "    # 4) Reorder to isTest, fea1…fea24, sid, class\n",
    "    feature_cols = [f\"fea{i}\" for i in range(1, len(nat_cols)+1)]\n",
    "    df_long = df_long[['isTest'] + feature_cols + ['sid', 'class']]\n",
    "    # --- End header-renaming block ---\n",
    "\n",
    "    print(\"Phase 1 (long – corrected) sample:\")\n",
    "    print(df_long.head(), \"\\n\")\n",
    "    df_long.to_csv('Phase1_NATOPS_Combined_long.csv', index=False)\n",
    "\n",
    "    # Phase 2: clustering & ratio features\n",
    "    n_clusters = 10\n",
    "    feat_cols = [c for c in df_long.columns\n",
    "                 if c not in ('sid','split','class','time_step')]\n",
    "    df_clust = perform_clustering(df_long.copy(), feat_cols, n_clusters)\n",
    "    df_phase2 = compute_ratio_features(df_clust, n_clusters)\n",
    "\n",
    "    print(\"Phase 2 sample:\")\n",
    "    print(df_phase2.head(), \"\\n\")\n",
    "    df_phase2.to_csv('Phase2_NATOPS_AtomicUnits.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
