{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5a4fac",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdcf92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "\n",
    "def read_arff_file(filepath, dimension_name):\n",
    "    \"\"\"\n",
    "    Reads an ARFF file, converts the data to a DataFrame, and \n",
    "    renames the columns as '[dimension_name]_t1', '[dimension_name]_t2', etc.\n",
    "    \"\"\"\n",
    "    data, meta = arff.loadarff(filepath)\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Rename columns using the dimension name and time step index.\n",
    "    num_columns = df.shape[1]\n",
    "    df.columns = [f\"{dimension_name}_t{i+1}\" for i in range(num_columns)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938dafbc",
   "metadata": {},
   "source": [
    "# Creating the Table (Using Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12c62c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_folder(folder_path, dataset_name):\n",
    "    \"\"\"\n",
    "    Processes one dataset folder by reading ARFF files and combining \n",
    "    them into a table. For each file, the function determines the data\n",
    "    split (train or test) based on the filename and assigns a dimension name.\n",
    "    It horizontally concatenates data for the same split,\n",
    "    then vertically concatenates train and test data.\n",
    "    \"\"\"\n",
    "    # List only ARFF files (ignore .txt, .png, .jpg, etc.)\n",
    "    arff_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.arff')]\n",
    "    \n",
    "    # Lists to store DataFrames for train and test files separately.\n",
    "    train_dfs = []\n",
    "    test_dfs = []\n",
    "\n",
    "    for file in arff_files:\n",
    "        filepath = os.path.join(folder_path, file)\n",
    "        \n",
    "        # Determine if the file is train or test based on its filename.\n",
    "        if \"TRAIN\" in file.upper():\n",
    "            split_label = \"train\"\n",
    "        elif \"TEST\" in file.upper():\n",
    "            split_label = \"test\"\n",
    "        else:\n",
    "            print(f\"Warning: Could not determine split (TRAIN/TEST) from file: {file}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract the dimension name by removing the '_TRAIN' or '_TEST' portion.\n",
    "        base_name = os.path.splitext(file)[0]  # e.g., \"NATOPSDimension1_TRAIN\"\n",
    "        if \"_TRAIN\" in base_name.upper():\n",
    "            dimension_name = base_name.upper().replace(\"_TRAIN\", \"\")\n",
    "        elif \"_TEST\" in base_name.upper():\n",
    "            dimension_name = base_name.upper().replace(\"_TEST\", \"\")\n",
    "        else:\n",
    "            dimension_name = base_name\n",
    "        \n",
    "        # For readability, convert the name to Title Case.\n",
    "        dimension_name = dimension_name.title()\n",
    "        \n",
    "        # Read the ARFF file into a DataFrame for this sensor dimension.\n",
    "        df_dim = read_arff_file(filepath, dimension_name)\n",
    "        # Add the split information to this DataFrame.\n",
    "        df_dim['split'] = split_label\n",
    "        \n",
    "        # Append to the appropriate list.\n",
    "        if split_label == \"train\":\n",
    "            train_dfs.append(df_dim)\n",
    "        else:\n",
    "            test_dfs.append(df_dim)\n",
    "    \n",
    "    # Concatenate data horizontally for the training part.\n",
    "    if train_dfs:\n",
    "        df_train = pd.concat(train_dfs, axis=1)\n",
    "        # Remove duplicate columns (e.g., multiple \"split\" columns).\n",
    "        df_train = df_train.loc[:, ~df_train.columns.duplicated()]\n",
    "        # Insert sample ID column.\n",
    "        df_train.insert(0, 'sid', range(1, len(df_train) + 1))\n",
    "    else:\n",
    "        df_train = pd.DataFrame()\n",
    "    \n",
    "    # Concatenate data horizontally for the testing part.\n",
    "    if test_dfs:\n",
    "        df_test = pd.concat(test_dfs, axis=1)\n",
    "        df_test = df_test.loc[:, ~df_test.columns.duplicated()]\n",
    "        df_test.insert(0, 'sid', range(1, len(df_test) + 1))\n",
    "    else:\n",
    "        df_test = pd.DataFrame()\n",
    "    \n",
    "    # Vertically stack the train and test DataFrames.\n",
    "    df_dataset = pd.concat([df_train, df_test], ignore_index=True)\n",
    "    \n",
    "    # Optionally add a column to track the dataset source.\n",
    "    df_dataset[\"dataset\"] = dataset_name\n",
    "    \n",
    "    return df_dataset\n",
    "\n",
    "def create_final_table():\n",
    "    \"\"\"\n",
    "    Creates the final table by processing multiple dataset folders.\n",
    "    For this example, we process the 'JapaneseVowels' and 'NATOPS' folders\n",
    "    under 'Phase1_Data' and then combine them.\n",
    "    \"\"\"\n",
    "    base_folder = 'Phase1_Data'\n",
    "    \n",
    "    # Define dataset folders (adjust as needed).\n",
    "    dataset_folders = {\n",
    "        \"JapaneseVowels\": os.path.join(base_folder, \"JapaneseVowels\"),\n",
    "        \"NATOPS\": os.path.join(base_folder, \"NATOPS\")\n",
    "    }\n",
    "    \n",
    "    all_datasets = []\n",
    "    for dataset_name, folder_path in dataset_folders.items():\n",
    "        if os.path.exists(folder_path):\n",
    "            df_dataset = process_dataset_folder(folder_path, dataset_name)\n",
    "            all_datasets.append(df_dataset)\n",
    "        else:\n",
    "            print(f\"Warning: Folder not found for dataset: {dataset_name}\")\n",
    "    \n",
    "    # Combine all dataset tables into one master DataFrame.\n",
    "    final_table = pd.concat(all_datasets, ignore_index=True) if all_datasets else pd.DataFrame()\n",
    "    return final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f8935c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd3c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data):\n",
    "    \"\"\"\n",
    "    Placeholder for model training.\n",
    "    In future phases, this function might preprocess features,\n",
    "    split into train/test sets further, and train a machine learning model.\n",
    "    \"\"\"\n",
    "    print(\"Training phase not implemented in Phase 1.\")\n",
    "    # Replace the following line with actual training code in future phases.\n",
    "    model = None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a84f03",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba793745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final table saved to Phase1_Combined.csv\n",
      "First 5 rows of the final table:\n",
      "   sid  Japanesevowelsdimension10_t1  Japanesevowelsdimension10_t2  \\\n",
      "0    1                     -0.306756                     -0.289431   \n",
      "1    2                     -0.173138                     -0.181910   \n",
      "2    3                     -0.237630                     -0.231087   \n",
      "3    4                      0.028707                      0.038970   \n",
      "4    5                     -0.115333                     -0.106838   \n",
      "\n",
      "   Japanesevowelsdimension10_t3  Japanesevowelsdimension10_t4  \\\n",
      "0                     -0.314894                     -0.323267   \n",
      "1                     -0.127751                     -0.182744   \n",
      "2                     -0.224317                     -0.208580   \n",
      "3                      0.005654                     -0.053426   \n",
      "4                     -0.125721                     -0.159460   \n",
      "\n",
      "   Japanesevowelsdimension10_t5  Japanesevowelsdimension10_t6  \\\n",
      "0                     -0.351171                     -0.364329   \n",
      "1                     -0.220531                     -0.256385   \n",
      "2                     -0.219516                     -0.247160   \n",
      "3                     -0.158885                     -0.253737   \n",
      "4                     -0.178367                     -0.176394   \n",
      "\n",
      "   Japanesevowelsdimension10_t7  Japanesevowelsdimension10_t8  \\\n",
      "0                     -0.358107                     -0.340491   \n",
      "1                     -0.186200                     -0.136580   \n",
      "2                     -0.244240                     -0.344519   \n",
      "3                     -0.336853                     -0.352695   \n",
      "4                     -0.193288                     -0.164090   \n",
      "\n",
      "   Japanesevowelsdimension10_t9  ...  Natopsdimension9_t45  \\\n",
      "0                     -0.355996  ...                   NaN   \n",
      "1                     -0.229586  ...                   NaN   \n",
      "2                     -0.451099  ...                   NaN   \n",
      "3                     -0.386457  ...                   NaN   \n",
      "4                     -0.120764  ...                   NaN   \n",
      "\n",
      "   Natopsdimension9_t46  Natopsdimension9_t47  Natopsdimension9_t48  \\\n",
      "0                   NaN                   NaN                   NaN   \n",
      "1                   NaN                   NaN                   NaN   \n",
      "2                   NaN                   NaN                   NaN   \n",
      "3                   NaN                   NaN                   NaN   \n",
      "4                   NaN                   NaN                   NaN   \n",
      "\n",
      "   Natopsdimension9_t49  Natopsdimension9_t50  Natopsdimension9_t51  \\\n",
      "0                   NaN                   NaN                   NaN   \n",
      "1                   NaN                   NaN                   NaN   \n",
      "2                   NaN                   NaN                   NaN   \n",
      "3                   NaN                   NaN                   NaN   \n",
      "4                   NaN                   NaN                   NaN   \n",
      "\n",
      "   Natopsdimension9_t52  Natops_t1  Natops_t2  \n",
      "0                   NaN        NaN        NaN  \n",
      "1                   NaN        NaN        NaN  \n",
      "2                   NaN        NaN        NaN  \n",
      "3                   NaN        NaN        NaN  \n",
      "4                   NaN        NaN        NaN  \n",
      "\n",
      "[5 rows x 1615 columns]\n",
      "Training phase not implemented in Phase 1.\n",
      "Testing phase not implemented in Phase 1.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Create the final table by combining data from the dataset folders\n",
    "    final_table = create_final_table()\n",
    "\n",
    "    # Save the final table to a CSV file.\n",
    "    output_filename = 'Phase1_Combined.csv'\n",
    "    final_table.to_csv(output_filename, index=False)\n",
    "    print(f\"Final table saved to {output_filename}\")\n",
    "\n",
    "    # Print the first 5 rows for a quick check.\n",
    "    print(\"First 5 rows of the final table:\")\n",
    "    print(final_table.head(5))\n",
    "\n",
    "    # Placeholder for the training phase.\n",
    "    model = train_model(final_table)\n",
    "\n",
    "    # Placeholder for the testing phase.\n",
    "    test_model(model, final_table)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
